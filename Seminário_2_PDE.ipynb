{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o9a3TV-LeHj"
      },
      "source": [
        "# **A Hadoop based platform for natural language processing of web pages and documents**\n",
        "</p> No projeto a seguir, apresentaremos uma plataforma baseada em Hadoop no processamento de linguagem natural em tweets ofensivos. <p>\n",
        "</p> O intuito do projeto é fazer um WordCount com palavras específicas, incluindo palavras compostas, dentro de uma base de dados. Os dados utilizados são disponbilizados para estudos de forma aberta. <p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw-rLBpB3cls"
      },
      "outputs": [],
      "source": [
        "# download do Hadoop pelo site da Apache e \n",
        "# cópia do arquivo para uma pasta /usr/local do ambiente Colab\n",
        "\n",
        "!wget https://archive.apache.org/dist/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
        "\n",
        "!tar -xzvf hadoop-3.3.0.tar.gz\n",
        "\n",
        "!cp -r hadoop-3.3.0/ /usr/local/\n",
        "\n",
        "# configuração do Java no Google Colab\n",
        "# esta configuração viabiliza a execução mais facilitada do Hadoop\n",
        "# para isso, usamos o seguinte script em python\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64/\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criando um diretório para guardar os arquivos para processar os dados com o Hadoop"
      ],
      "metadata": {
        "id": "iOYITR3atlaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49zghXCR7PvL"
      },
      "outputs": [],
      "source": [
        "!$HADOOP_HOME/bin/hadoop fs -mkdir hadoop_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fazendo download das bases de dados selecionadas"
      ],
      "metadata": {
        "id": "wBZehgTLtwja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0ygqYjF796n"
      },
      "outputs": [],
      "source": [
        "!wget https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0&d=1\n",
        "!wget https://drive.google.com/u/0/uc?id=1-ybjDIfP8pv81uZHfwKL5D1hZQn3cJB7&export=download"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descompactando os arquivos baixados"
      ],
      "metadata": {
        "id": "mQfrVPZ3tdUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbv0KaoT9SHP"
      },
      "outputs": [],
      "source": [
        "!unzip OLIDv1.0.zip?attredirects=0 -d OLID_files\n",
        "!unzip uc?id=1-ybjDIfP8pv81uZHfwKL5D1hZQn3cJB7 -d OLID_files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecionando os dados de interesse"
      ],
      "metadata": {
        "id": "-OJt8XVUt0F0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.read_csv('OLID_files/olid-training-v1.0.tsv', sep='\\t'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qb5cZb2zej7",
        "outputId": "db60d86b-9019-4676-f225-d51e3c047f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          id                                              tweet subtask_a  \\\n",
            "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
            "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
            "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
            "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
            "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
            "...      ...                                                ...       ...   \n",
            "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
            "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
            "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
            "13238  27429                                        @USER Pussy       OFF   \n",
            "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
            "\n",
            "      subtask_b subtask_c  \n",
            "0           UNT       NaN  \n",
            "1           TIN       IND  \n",
            "2           NaN       NaN  \n",
            "3           UNT       NaN  \n",
            "4           NaN       NaN  \n",
            "...         ...       ...  \n",
            "13235       TIN       IND  \n",
            "13236       NaN       NaN  \n",
            "13237       TIN       OTH  \n",
            "13238       UNT       NaN  \n",
            "13239       NaN       NaN  \n",
            "\n",
            "[13240 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.read_csv('OLID_files/bad-words.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RTg5SJMzlzM",
        "outputId": "e4402bc8-3bf9-4749-8a20-7462e800e4d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             jigaboo\n",
            "0     mound of venus\n",
            "1           asslover\n",
            "2                s&m\n",
            "3              queaf\n",
            "4         whitetrash\n",
            "...              ...\n",
            "1611           cocky\n",
            "1612     transsexual\n",
            "1613      unfuckable\n",
            "1614      bestiality\n",
            "1615      cocklicker\n",
            "\n",
            "[1616 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOpTSUMQHwhp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tweets = pd.read_csv('OLID_files/olid-training-v1.0.tsv', sep='\\t')['tweet']\n",
        "\n",
        "badwords = pd.read_csv('OLID_files/bad-words.csv')['jigaboo']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalando biblioteca para decodar palavras com caracteres diferentes"
      ],
      "metadata": {
        "id": "UECW8P9Yt6UF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Bn3qRT2Ip3U"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode\n",
        "from unidecode import unidecode\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processando os tweets, passando todas as palavras para lower case e mantendo apenas letras."
      ],
      "metadata": {
        "id": "BPakJCbWuEUX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4FR9c0eIzzv"
      },
      "outputs": [],
      "source": [
        "for item in range(0, len(tweets)):\n",
        "  tweets[item] = unidecode(tweets[item]).lower()\n",
        "  tweets[item] = re.sub(r'[^a-zA-Z]', ' ', tweets[item])\n",
        "  tweets[item] = tweets[item].replace(' user ', '')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gerando um arquivo de texto para cada base de dados, com os dados de interesse."
      ],
      "metadata": {
        "id": "QSPMh8OGuPrt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LIAmRbOPLzC"
      },
      "outputs": [],
      "source": [
        "with open('OLID_files/tweets.txt', 'w') as f:\n",
        "  for line in tweets:\n",
        "    f.write(line)\n",
        "    f.write('\\n')\n",
        "  f.close()\n",
        "\n",
        "with open('OLID_files/badwords.txt', 'w') as f:\n",
        "  for badword in badwords:\n",
        "    f.write(badword)\n",
        "    f.write('\\n')\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz97UeZXLnlN"
      },
      "outputs": [],
      "source": [
        "# !rm hadoop_data/badwords.txt\n",
        "# !rm hadoop_data/tweets.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copiando os arquivos pra partição do Hadoop"
      ],
      "metadata": {
        "id": "4mK5YpFyuYHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw0cV9Be_Ig8"
      },
      "outputs": [],
      "source": [
        "!$HADOOP_HOME/bin/hadoop fs -put /content/OLID_files/tweets.txt hadoop_data/\n",
        "!$HADOOP_HOME/bin/hadoop fs -put /content/OLID_files/badwords.txt hadoop_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testando se as cópias foram criadas com sucesso."
      ],
      "metadata": {
        "id": "UnyffivvuiEp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqCD0yhFAtCW",
        "outputId": "c0e7fd78-187d-419e-fe5e-77b0c604254f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 items\n",
            "drwxr-xr-x   - root root       4096 2022-08-31 13:59 hadoop_data/.ipynb_checkpoints\n",
            "-rw-r--r--   1 root root      13649 2022-08-31 14:57 hadoop_data/badwords.txt\n",
            "-rw-r--r--   1 root root    1476392 2022-08-31 14:57 hadoop_data/tweets.txt\n",
            "z dost and how much she needs that touch to comfort her restless head right now is all evident in this one freaking scene  bow to these amazing actors  jenshad is major actors and couple goals  adiya  bepannaah\n",
            "billy you have a short memory  obama tried to get in commonsense gun control is especially after sandyhook  the parents even came in and begged congress to do something about automatic weapons  but the nra had such a hold on congress democrats and repugs nothing was done\n",
            "but gun control   \n",
            "she is not the brightest light on the tree \n",
            " if i say you are mad now you will say i m tired of you \n",
            "retweet complete  amp  followed all patriots \n",
            "sometimes i get strong vibes from people and this man s vibe is tens of millions of murders   he is more dangerous than dt \n",
            "benidorm   creamfields   maga    not too shabby of a summer\n",
            "and why report this garbage   we don t give a crap \n",
            "pussy\n",
            " spanishrevenge vs   justice  humanrights and  freedomofexpression  spain is a   fakedemocracy  cddr  shameonspain  wakeupeurope url\n",
            "um\n",
            "cemetery\n",
            "rigger\n",
            "ethical slut\n",
            "dolcett\n",
            "ejaculating \n",
            "babe\n",
            "cumquat\n",
            "eatpussy\n",
            "phuk\n",
            "givehead\n",
            "drunk\n",
            "phukking\n",
            "shiteater\n",
            "catholic\n",
            "hooker\n",
            "cocksuck \n",
            "cocksucker\n",
            "ball kicking\n",
            "fuckyou\n",
            "crappy\n",
            "feltching\n",
            "mideast\n",
            "ecstacy\n",
            "ribbed\n",
            "dog style\n",
            "interracial\n",
            "tongue in a\n",
            "pocha\n",
            "skankywhore\n",
            "pu55y\n",
            "motherfuckings\n",
            "piker\n",
            "peepshow\n",
            "jap\n",
            "yiffy\n",
            "tongethruster\n",
            "nigger's\n",
            "breastlover\n",
            "stroke\n",
            "twobitwhore\n",
            "shits\n",
            "israel's\n",
            "jerkoff\n",
            "bullet vibe\n",
            "assassinate\n",
            "killed\n",
            "pocketpool\n",
            "whacker\n",
            "wtf\n",
            "barf\n",
            "juggalo\n",
            "negro\n",
            "spick\n",
            "gyppy\n",
            "nymph\n",
            "snigger's\n",
            "violence\n",
            "lovemuscle\n",
            "dago\n",
            "feces\n",
            "booty\n",
            "niggers\n",
            "shitter\n",
            "sodomy\n",
            "hussy\n",
            "pisspig\n",
            "coprophilia\n",
            "christian\n",
            "pimped\n",
            "boob\n",
            "breast\n",
            "sperm\n",
            "coloured\n",
            "redlight\n",
            "blacks\n",
            "orga\n",
            "bumblefuck\n",
            "mams\n",
            "slavedriver\n",
            "killing\n",
            "uptheass\n",
            "bestial\n",
            "sweetness\n",
            "heeb\n",
            "piccaninny\n",
            "pot\n",
            "honk\n",
            "jizjuice\n",
            "fuc\n",
            "nignog\n",
            "mgger\n",
            "sexing\n",
            "virginbreaker\n",
            "samckdaddy\n",
            "masterblaster\n",
            "heterosexual\n",
            "jigger \n",
            "blowjob\n",
            "lovegun\n",
            "shitstain\n",
            "spank\n",
            "hiv\n",
            "lesbain\n",
            "mad\n",
            "sniggers\n",
            "jizm \n",
            "testicle\n",
            "ball sucking\n",
            "dragqween\n",
            "guro\n",
            "pubic\n",
            "titfuckin\n",
            "moneyshot\n",
            "camslut\n",
            "bountybar\n",
            "assklown\n",
            "cocky\n",
            "transsexual\n",
            "unfuckable\n",
            "bestiality\n",
            "cocklicker\n"
          ]
        }
      ],
      "source": [
        "!$HADOOP_HOME/bin/hadoop fs -ls hadoop_data/\n",
        "\n",
        "!$HADOOP_HOME/bin/hadoop fs -tail hadoop_data/tweets.txt\n",
        "!$HADOOP_HOME/bin/hadoop fs -tail hadoop_data/badwords.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dando permissão aos códigos desenvolvidos para map e reduce."
      ],
      "metadata": {
        "id": "nrBcg697umS9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ohm7L05CjQq"
      },
      "outputs": [],
      "source": [
        "!chmod 755 mapper.py reducer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClCH5WK0KFHi"
      },
      "outputs": [],
      "source": [
        "# !rm -r output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adaptando os códigos mapper e reducer para substituirem as funções do Hadoop, para rodarem em um cluster do Hadoop"
      ],
      "metadata": {
        "id": "E4-d2bQ9wr_5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwMIAo8V57gw",
        "outputId": "1a7cbbd5-cf19-45cd-ed7c-c6b407acec6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-31 14:57:37,188 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-08-31 14:57:37,591 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-08-31 14:57:37,592 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-08-31 14:57:37,662 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-08-31 14:57:37,971 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-08-31 14:57:37,998 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-08-31 14:57:38,317 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2128545062_0001\n",
            "2022-08-31 14:57:38,317 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-08-31 14:57:38,786 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local2128545062_0001_94b81df5-a542-4fbc-987d-2cb33b327525/mapper.py\n",
            "2022-08-31 14:57:38,829 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local2128545062_0001_6ef4e3ce-44b6-485c-b70e-0eddbbc8a6a1/reducer.py\n",
            "2022-08-31 14:57:38,921 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-08-31 14:57:38,923 INFO mapreduce.Job: Running job: job_local2128545062_0001\n",
            "2022-08-31 14:57:38,929 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-08-31 14:57:38,932 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-08-31 14:57:38,936 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-08-31 14:57:38,937 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-08-31 14:57:38,999 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-08-31 14:57:39,003 INFO mapred.LocalJobRunner: Starting task: attempt_local2128545062_0001_m_000000_0\n",
            "2022-08-31 14:57:39,035 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-08-31 14:57:39,036 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-08-31 14:57:39,076 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-08-31 14:57:39,089 INFO mapred.MapTask: Processing split: file:/content/hadoop_data/tweets.txt:0+1476392\n",
            "2022-08-31 14:57:39,114 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-08-31 14:57:39,356 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-08-31 14:57:39,356 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-08-31 14:57:39,356 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-08-31 14:57:39,356 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-08-31 14:57:39,356 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-08-31 14:57:39,360 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-08-31 14:57:39,373 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2022-08-31 14:57:39,381 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-08-31 14:57:39,382 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-08-31 14:57:39,382 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-08-31 14:57:39,383 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-08-31 14:57:39,384 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-08-31 14:57:39,384 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-08-31 14:57:39,384 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-08-31 14:57:39,385 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-08-31 14:57:39,385 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-08-31 14:57:39,387 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-08-31 14:57:39,387 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-08-31 14:57:39,388 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-08-31 14:57:39,427 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 14:57:39,427 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 14:57:39,430 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 14:57:39,454 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 14:57:39,928 INFO mapreduce.Job: Job job_local2128545062_0001 running in uber mode : false\n",
            "2022-08-31 14:57:39,929 INFO mapreduce.Job:  map 0% reduce 0%\n",
            "2022-08-31 14:57:51,054 INFO mapred.LocalJobRunner: file:/content/hadoop_data/tweets.txt:0+1476392 > map\n",
            "2022-08-31 14:57:51,947 INFO mapreduce.Job:  map 6% reduce 0%\n",
            "2022-08-31 14:58:03,056 INFO mapred.LocalJobRunner: file:/content/hadoop_data/tweets.txt:0+1476392 > map\n",
            "2022-08-31 14:58:03,958 INFO mapreduce.Job:  map 12% reduce 0%\n",
            "2022-08-31 14:58:22,393 INFO streaming.PipeMapRed: Records R/W=2374/1\n",
            "2022-08-31 14:58:33,058 INFO mapred.LocalJobRunner: Records R/W=2374/1 > map\n",
            "2022-08-31 14:58:33,981 INFO mapreduce.Job:  map 18% reduce 0%\n",
            "2022-08-31 14:59:03,060 INFO mapred.LocalJobRunner: Records R/W=2374/1 > map\n",
            "2022-08-31 14:59:04,006 INFO mapreduce.Job:  map 24% reduce 0%\n",
            "2022-08-31 14:59:06,946 INFO streaming.PipeMapRed: Records R/W=4720/1010\n",
            "2022-08-31 14:59:15,062 INFO mapred.LocalJobRunner: Records R/W=4720/1010 > map\n",
            "2022-08-31 14:59:33,066 INFO mapred.LocalJobRunner: Records R/W=4720/1010 > map\n",
            "2022-08-31 14:59:34,027 INFO mapreduce.Job:  map 30% reduce 0%\n",
            "2022-08-31 14:59:47,755 INFO streaming.PipeMapRed: Records R/W=5871/2017\n",
            "2022-08-31 14:59:57,068 INFO mapred.LocalJobRunner: Records R/W=5871/2017 > map\n",
            "2022-08-31 14:59:58,045 INFO mapreduce.Job:  map 36% reduce 0%\n",
            "2022-08-31 15:00:03,071 INFO mapred.LocalJobRunner: Records R/W=5871/2017 > map\n",
            "2022-08-31 15:00:33,075 INFO mapred.LocalJobRunner: Records R/W=5871/2017 > map\n",
            "2022-08-31 15:00:33,319 INFO streaming.PipeMapRed: Records R/W=8230/3012\n",
            "2022-08-31 15:00:34,069 INFO mapreduce.Job:  map 41% reduce 0%\n",
            "2022-08-31 15:00:45,077 INFO mapred.LocalJobRunner: Records R/W=8230/3012 > map\n",
            "2022-08-31 15:01:03,081 INFO mapred.LocalJobRunner: Records R/W=8230/3012 > map\n",
            "2022-08-31 15:01:03,088 INFO mapreduce.Job:  map 47% reduce 0%\n",
            "2022-08-31 15:01:20,808 INFO streaming.PipeMapRed: Records R/W=9405/4018\n",
            "2022-08-31 15:01:25,289 INFO streaming.PipeMapRed: R/W/S=10000/5033/0 in:44=10000/225 [rec/s] out:22=5033/225 [rec/s]\n",
            "2022-08-31 15:01:27,084 INFO mapred.LocalJobRunner: Records R/W=9405/4018 > map\n",
            "2022-08-31 15:01:27,108 INFO mapreduce.Job:  map 53% reduce 0%\n",
            "2022-08-31 15:01:33,086 INFO mapred.LocalJobRunner: Records R/W=9405/4018 > map\n",
            "2022-08-31 15:02:00,610 INFO streaming.PipeMapRed: Records R/W=11776/5034\n",
            "2022-08-31 15:02:03,088 INFO mapred.LocalJobRunner: Records R/W=11776/5034 > map\n",
            "2022-08-31 15:02:03,127 INFO mapreduce.Job:  map 59% reduce 0%\n",
            "2022-08-31 15:02:09,092 INFO mapred.LocalJobRunner: Records R/W=11776/5034 > map\n",
            "2022-08-31 15:02:33,095 INFO mapred.LocalJobRunner: Records R/W=11776/5034 > map\n",
            "2022-08-31 15:02:33,142 INFO mapreduce.Job:  map 65% reduce 0%\n",
            "2022-08-31 15:02:42,725 INFO streaming.PipeMapRed: Records R/W=12930/6032\n",
            "2022-08-31 15:02:51,099 INFO mapred.LocalJobRunner: Records R/W=12930/6032 > map\n",
            "2022-08-31 15:03:03,101 INFO mapred.LocalJobRunner: Records R/W=12930/6032 > map\n",
            "2022-08-31 15:03:03,156 INFO mapreduce.Job:  map 67% reduce 0%\n",
            "2022-08-31 15:03:17,945 INFO streaming.PipeMapRed: Records R/W=13240/7047\n",
            "2022-08-31 15:03:17,952 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-08-31 15:03:17,953 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-08-31 15:03:17,954 INFO mapred.LocalJobRunner: Records R/W=12930/6032 > map\n",
            "2022-08-31 15:03:17,954 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-08-31 15:03:17,954 INFO mapred.MapTask: Spilling map output\n",
            "2022-08-31 15:03:17,954 INFO mapred.MapTask: bufstart = 0; bufend = 64504; bufvoid = 104857600\n",
            "2022-08-31 15:03:17,954 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26182764(104731056); length = 31633/6553600\n",
            "2022-08-31 15:03:18,021 INFO mapred.MapTask: Finished spill 0\n",
            "2022-08-31 15:03:18,028 INFO mapred.Task: Task:attempt_local2128545062_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-08-31 15:03:18,030 INFO mapred.LocalJobRunner: Records R/W=13240/7047\n",
            "2022-08-31 15:03:18,030 INFO mapred.Task: Task 'attempt_local2128545062_0001_m_000000_0' done.\n",
            "2022-08-31 15:03:18,038 INFO mapred.Task: Final Counters for attempt_local2128545062_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1631939\n",
            "\t\tFILE: Number of bytes written=840554\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13240\n",
            "\t\tMap output records=7909\n",
            "\t\tMap output bytes=64504\n",
            "\t\tMap output materialized bytes=80328\n",
            "\t\tInput split bytes=88\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=7909\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=284164096\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1487940\n",
            "2022-08-31 15:03:18,039 INFO mapred.LocalJobRunner: Finishing task: attempt_local2128545062_0001_m_000000_0\n",
            "2022-08-31 15:03:18,039 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-08-31 15:03:18,043 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-08-31 15:03:18,048 INFO mapred.LocalJobRunner: Starting task: attempt_local2128545062_0001_r_000000_0\n",
            "2022-08-31 15:03:18,058 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-08-31 15:03:18,058 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-08-31 15:03:18,059 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-08-31 15:03:18,064 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@14c7990b\n",
            "2022-08-31 15:03:18,066 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-08-31 15:03:18,090 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2384042240, maxSingleShuffleLimit=596010560, mergeThreshold=1573467904, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-08-31 15:03:18,093 INFO reduce.EventFetcher: attempt_local2128545062_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-08-31 15:03:18,137 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2128545062_0001_m_000000_0 decomp: 80324 len: 80328 to MEMORY\n",
            "2022-08-31 15:03:18,141 INFO reduce.InMemoryMapOutput: Read 80324 bytes from map-output for attempt_local2128545062_0001_m_000000_0\n",
            "2022-08-31 15:03:18,146 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 80324, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->80324\n",
            "2022-08-31 15:03:18,147 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-08-31 15:03:18,150 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-08-31 15:03:18,150 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-08-31 15:03:18,157 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-08-31 15:03:18,157 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 80313 bytes\n",
            "2022-08-31 15:03:18,163 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2022-08-31 15:03:18,181 INFO reduce.MergeManagerImpl: Merged 1 segments, 80324 bytes to disk to satisfy reduce memory limit\n",
            "2022-08-31 15:03:18,182 INFO reduce.MergeManagerImpl: Merging 1 files, 80328 bytes from disk\n",
            "2022-08-31 15:03:18,183 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-08-31 15:03:18,183 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-08-31 15:03:18,184 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 80313 bytes\n",
            "2022-08-31 15:03:18,185 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-08-31 15:03:18,196 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2022-08-31 15:03:18,224 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-08-31 15:03:18,227 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-08-31 15:03:18,255 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 15:03:18,255 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 15:03:18,257 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 15:03:18,272 INFO streaming.PipeMapRed: R/W/S=1000/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-08-31 15:03:18,379 INFO streaming.PipeMapRed: Records R/W=7909/1\n",
            "2022-08-31 15:03:18,387 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-08-31 15:03:18,388 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-08-31 15:03:18,389 INFO mapred.Task: Task:attempt_local2128545062_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-08-31 15:03:18,390 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-08-31 15:03:18,390 INFO mapred.Task: Task attempt_local2128545062_0001_r_000000_0 is allowed to commit now\n",
            "2022-08-31 15:03:18,392 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2128545062_0001_r_000000_0' to file:/content/output\n",
            "2022-08-31 15:03:18,393 INFO mapred.LocalJobRunner: Records R/W=7909/1 > reduce\n",
            "2022-08-31 15:03:18,393 INFO mapred.Task: Task 'attempt_local2128545062_0001_r_000000_0' done.\n",
            "2022-08-31 15:03:18,394 INFO mapred.Task: Final Counters for attempt_local2128545062_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1792627\n",
            "\t\tFILE: Number of bytes written=924756\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=406\n",
            "\t\tReduce shuffle bytes=80328\n",
            "\t\tReduce input records=7909\n",
            "\t\tReduce output records=406\n",
            "\t\tSpilled Records=7909\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=23\n",
            "\t\tTotal committed heap usage (bytes)=284164096\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=3874\n",
            "2022-08-31 15:03:18,394 INFO mapred.LocalJobRunner: Finishing task: attempt_local2128545062_0001_r_000000_0\n",
            "2022-08-31 15:03:18,394 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-08-31 15:03:19,164 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-08-31 15:03:19,164 INFO mapreduce.Job: Job job_local2128545062_0001 completed successfully\n",
            "2022-08-31 15:03:19,180 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3424566\n",
            "\t\tFILE: Number of bytes written=1765310\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=13240\n",
            "\t\tMap output records=7909\n",
            "\t\tMap output bytes=64504\n",
            "\t\tMap output materialized bytes=80328\n",
            "\t\tInput split bytes=88\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=406\n",
            "\t\tReduce shuffle bytes=80328\n",
            "\t\tReduce input records=7909\n",
            "\t\tReduce output records=406\n",
            "\t\tSpilled Records=15818\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=23\n",
            "\t\tTotal committed heap usage (bytes)=568328192\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1487940\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=3874\n",
            "2022-08-31 15:03:19,180 INFO streaming.StreamJob: Output directory: /content/output\n"
          ]
        }
      ],
      "source": [
        "!$HADOOP_HOME/bin/hadoop jar /content/hadoop-3.3.0/share/hadoop/tools/lib/hadoop-streaming-3.3.0.jar \\\n",
        "-files mapper.py,reducer.py \\\n",
        "-mapper mapper.py \\\n",
        "-reducer reducer.py \\\n",
        "-input /content/hadoop_data/tweets.txt -output /content/output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printando os dados de saída do output"
      ],
      "metadata": {
        "id": "OQhp5FVzwzOe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIhay4fuOwM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072e94de-92e9-408d-a251-b336d106c171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abortion\t39\n",
            "abuse\t37\n",
            "addict\t4\n",
            "addicts\t1\n",
            "adult\t11\n",
            "africa\t6\n",
            "african\t9\n",
            "amateur\t1\n",
            "american\t131\n",
            "anal\t1\n",
            "angie\t2\n",
            "angry\t18\n",
            "arsehole\t1\n",
            "asian\t5\n",
            "ass\t183\n",
            "assassin\t1\n",
            "assassinate\t3\n",
            "assassination\t6\n",
            "assault\t65\n",
            "asses\t7\n",
            "asshole\t20\n",
            "assholes\t9\n",
            "attack\t63\n",
            "australian\t8\n",
            "babe\t8\n",
            "babies\t24\n",
            "balls\t12\n",
            "banging\t1\n",
            "baptist\t2\n",
            "barf\t1\n",
            "bastard\t5\n",
            "beast\t7\n",
            "bi\t4\n",
            "bible\t18\n",
            "bigger\t16\n",
            "bimbos\t1\n",
            "bitch\t100\n",
            "bitches\t15\n",
            "bitching\t3\n",
            "bitchy\t1\n",
            "black\t141\n",
            "blackout\t2\n",
            "blacks\t10\n",
            "blind\t19\n",
            "blow\t15\n",
            "bomb\t11\n",
            "bombs\t3\n",
            "bondage\t1\n",
            "boner\t2\n",
            "boob\t1\n",
            "boobies\t1\n",
            "boobs\t7\n",
            "boom\t3\n",
            "booty\t9\n",
            "breast\t1\n",
            "brothel\t1\n",
            "bullcrap\t1\n",
            "bullshit\t56\n",
            "bunga\t1\n",
            "buried\t3\n",
            "burn\t19\n",
            "butt\t24\n",
            "butthead\t2\n",
            "buttmunch\t1\n",
            "canadian\t18\n",
            "cancer\t17\n",
            "catholic\t13\n",
            "catholics\t4\n",
            "chin\t2\n",
            "chinese\t13\n",
            "christ\t12\n",
            "christian\t33\n",
            "church\t27\n",
            "cigarette\t1\n",
            "cock\t7\n",
            "cocks\t2\n",
            "cocktail\t1\n",
            "cocky\t1\n",
            "color\t24\n",
            "colored\t3\n",
            "commie\t9\n",
            "communist\t24\n",
            "condom\t1\n",
            "conservative\t115\n",
            "conspiracy\t17\n",
            "cornhole\t1\n",
            "corruption\t28\n",
            "crack\t4\n",
            "crap\t47\n",
            "crapper\t1\n",
            "crappy\t3\n",
            "crash\t7\n",
            "crime\t69\n",
            "crimes\t29\n",
            "criminal\t43\n",
            "criminals\t57\n",
            "cum\t5\n",
            "cumm\t1\n",
            "cumming\t2\n",
            "cunt\t6\n",
            "dammit\t2\n",
            "damn\t60\n",
            "dead\t67\n",
            "death\t67\n",
            "deep throat\t4\n",
            "demon\t9\n",
            "desire\t2\n",
            "destroy\t54\n",
            "devil\t14\n",
            "dick\t27\n",
            "dickbrain\t1\n",
            "dickhead\t1\n",
            "dickweed\t1\n",
            "die\t34\n",
            "died\t31\n",
            "dies\t6\n",
            "dildo\t3\n",
            "dipshit\t3\n",
            "dipstick\t1\n",
            "dirty\t28\n",
            "disease\t2\n",
            "diseases\t1\n",
            "disturbed\t6\n",
            "dive\t2\n",
            "dong\t4\n",
            "doom\t2\n",
            "dope\t12\n",
            "drug\t22\n",
            "drunk\t28\n",
            "dumb\t52\n",
            "dumbass\t4\n",
            "dumbfuck\t1\n",
            "dyke\t1\n",
            "eat my ass\t1\n",
            "enemy\t15\n",
            "erect\t1\n",
            "escort\t3\n",
            "ethnic\t2\n",
            "european\t9\n",
            "execute\t2\n",
            "executed\t1\n",
            "faggot\t1\n",
            "failed\t23\n",
            "failure\t26\n",
            "fairy\t1\n",
            "faith\t14\n",
            "fart\t3\n",
            "fat\t29\n",
            "fear\t42\n",
            "fecal\t1\n",
            "fight\t70\n",
            "fingering\t1\n",
            "fire\t35\n",
            "firing\t1\n",
            "fornicate\t1\n",
            "fraud\t29\n",
            "fu\t4\n",
            "fubar\t2\n",
            "fuck\t183\n",
            "fucked\t31\n",
            "fucker\t6\n",
            "fuckers\t2\n",
            "fuckin\t15\n",
            "fucking\t147\n",
            "fuckoff\t2\n",
            "fucks\t4\n",
            "fuk\t1\n",
            "funeral\t3\n",
            "gangsta\t1\n",
            "gay\t28\n",
            "geez\t4\n",
            "german\t8\n",
            "girl on\t1\n",
            "girls\t23\n",
            "god\t197\n",
            "goddamn\t3\n",
            "gross\t6\n",
            "gun\t1378\n",
            "hamas\t1\n",
            "hard core\t3\n",
            "hardcore\t6\n",
            "harder\t10\n",
            "hell\t80\n",
            "heroin\t1\n",
            "hijack\t1\n",
            "hillbillies\t3\n",
            "hitler\t18\n",
            "hiv\t4\n",
            "ho\t6\n",
            "hole\t17\n",
            "homicide\t10\n",
            "homosexual\t3\n",
            "hook\t7\n",
            "hooker\t1\n",
            "horny\t1\n",
            "horseshit\t3\n",
            "hostage\t3\n",
            "humping\t1\n",
            "idiot\t57\n",
            "illegal\t55\n",
            "interracial\t1\n",
            "israel\t21\n",
            "israeli\t3\n",
            "itch\t1\n",
            "jackoff\t1\n",
            "japanese\t5\n",
            "jerk off\t1\n",
            "jerkoff\t1\n",
            "jesus\t25\n",
            "jesuschrist\t1\n",
            "jewish\t10\n",
            "joint\t4\n",
            "kid\t31\n",
            "kill\t53\n",
            "killed\t45\n",
            "killer\t12\n",
            "killing\t37\n",
            "kills\t9\n",
            "kinky\t2\n",
            "kkk\t46\n",
            "knife\t7\n",
            "ky\t1\n",
            "laid\t2\n",
            "latin\t3\n",
            "lesbian\t6\n",
            "liberal\t222\n",
            "licker\t1\n",
            "lies\t97\n",
            "lingerie\t1\n",
            "liquor\t2\n",
            "loser\t17\n",
            "lowlife\t2\n",
            "lucifer\t1\n",
            "lynch\t3\n",
            "mad\t28\n",
            "mafia\t2\n",
            "marijuana\t5\n",
            "masterbate\t1\n",
            "masturbate\t1\n",
            "meth\t3\n",
            "mexican\t4\n",
            "milf\t1\n",
            "minority\t10\n",
            "molest\t2\n",
            "molestation\t1\n",
            "molester\t1\n",
            "moron\t15\n",
            "motherfucker\t1\n",
            "murder\t37\n",
            "murderer\t6\n",
            "muslim\t27\n",
            "naked\t3\n",
            "nasty\t7\n",
            "nazi\t48\n",
            "negroes\t2\n",
            "nigerian\t1\n",
            "nigga\t32\n",
            "niggah\t1\n",
            "niggaz\t1\n",
            "nigger\t4\n",
            "niggers\t1\n",
            "nude\t1\n",
            "nuke\t1\n",
            "oral\t1\n",
            "orgasim\t1\n",
            "osama\t1\n",
            "palestinian\t2\n",
            "pansies\t1\n",
            "pansy\t1\n",
            "panties\t1\n",
            "pedophile\t6\n",
            "pee\t3\n",
            "penis\t7\n",
            "penthouse\t2\n",
            "period\t17\n",
            "perv\t2\n",
            "piece of shit\t11\n",
            "piss\t11\n",
            "pissed\t7\n",
            "pisses\t5\n",
            "pissing\t9\n",
            "playboy\t1\n",
            "poof\t1\n",
            "poontang\t1\n",
            "poop\t4\n",
            "porn\t17\n",
            "pornography\t1\n",
            "pot\t18\n",
            "poverty\t11\n",
            "prick\t1\n",
            "propaganda\t27\n",
            "pros\t2\n",
            "prostitute\t3\n",
            "protestant\t1\n",
            "puke\t2\n",
            "pussies\t4\n",
            "pussy\t26\n",
            "rabbi\t1\n",
            "racial\t9\n",
            "racist\t108\n",
            "radical\t27\n",
            "radicals\t8\n",
            "randy\t2\n",
            "rape\t42\n",
            "raped\t12\n",
            "raping\t4\n",
            "rapist\t13\n",
            "redneck\t2\n",
            "reefer\t1\n",
            "refugee\t4\n",
            "reject\t3\n",
            "remains\t5\n",
            "republican\t78\n",
            "retard\t1\n",
            "retarded\t6\n",
            "robber\t1\n",
            "rump\t2\n",
            "satan\t17\n",
            "screw\t5\n",
            "scum\t16\n",
            "servant\t2\n",
            "sex\t54\n",
            "sexual\t59\n",
            "sexually\t16\n",
            "sexy\t27\n",
            "shat\t2\n",
            "shit\t361\n",
            "shite\t7\n",
            "shithead\t1\n",
            "shithouse\t1\n",
            "shits\t7\n",
            "shitted\t1\n",
            "shitter\t1\n",
            "shitting\t7\n",
            "shitty\t20\n",
            "shoot\t17\n",
            "shooting\t61\n",
            "sick\t82\n",
            "skank\t1\n",
            "slant\t1\n",
            "slapper\t1\n",
            "slaughter\t3\n",
            "slave\t9\n",
            "slimeball\t1\n",
            "slut\t3\n",
            "smack\t1\n",
            "smut\t3\n",
            "snatch\t1\n",
            "snot\t1\n",
            "sob\t4\n",
            "sonofabitch\t1\n",
            "sos\t1\n",
            "soviet\t3\n",
            "spank\t1\n",
            "spit\t4\n",
            "spunk\t1\n",
            "stroke\t1\n",
            "stroking\t1\n",
            "stupid\t122\n",
            "suck\t35\n",
            "sucker\t2\n",
            "sucks\t54\n",
            "suicide\t12\n",
            "swallow\t4\n",
            "sweetness\t1\n",
            "taboo\t1\n",
            "terror\t14\n",
            "terrorist\t48\n",
            "tied up\t1\n",
            "tit\t2\n",
            "tits\t2\n",
            "titties\t1\n",
            "titty\t1\n",
            "toilet\t3\n",
            "tongue\t4\n",
            "tosser\t1\n",
            "tranny\t2\n",
            "transvestite\t1\n",
            "turd\t4\n",
            "twat\t2\n",
            "uk\t48\n",
            "unfuckable\t1\n",
            "uterus\t1\n",
            "vagina\t2\n",
            "vatican\t5\n",
            "violence\t138\n",
            "virgin\t1\n",
            "vomit\t2\n",
            "wab\t1\n",
            "wanker\t1\n",
            "weapon\t18\n",
            "weenie\t1\n",
            "welfare\t12\n",
            "wet dream\t1\n",
            "whit\t3\n",
            "white power\t4\n",
            "whites\t6\n",
            "whitey\t2\n",
            "whore\t10\n",
            "willie\t7\n",
            "willy\t1\n",
            "wtf\t23\n",
            "xtc\t1\n",
            "xx\t2\n",
            "xxx\t3\n",
            "yankee\t1\n"
          ]
        }
      ],
      "source": [
        "!$HADOOP_HOME/bin/hadoop fs -cat output/part-00000"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pegando os dados do output do Hadoop e organizando na ordem decrescente de palavras usadas."
      ],
      "metadata": {
        "id": "V2tVKPVuxhOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/output/part-00000', 'r') as f:\n",
        "  txt = {word.strip() for word in f}\n",
        "  f.close()\n",
        "\n",
        "badword = []\n",
        "qtd = []\n",
        "\n",
        "for item in txt:\n",
        "  bd_aux, qtd_aux = item.split('\\t')\n",
        "  badword.insert(len(badword), bd_aux)\n",
        "  qtd.insert(len(qtd), int(qtd_aux))\n",
        "\n",
        "data = pd.DataFrame([badword, qtd], index=['Badwords', 'Count']).T.sort_values(by='Count', ascending=False)\n",
        "print(data.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDgClUZfeEtX",
        "outputId": "74aba0df-7f29-411d-bc29-b6aaafb34e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Badwords Count\n",
            "320       gun  1378\n",
            "346      shit   361\n",
            "325   liberal   222\n",
            "392       god   197\n",
            "15       fuck   183\n",
            "229       ass   183\n",
            "95    fucking   147\n",
            "60      black   141\n",
            "7    violence   138\n",
            "366  american   131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapper"
      ],
      "metadata": {
        "id": "TvMtMbUowlWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6984cdT8Lbvo"
      },
      "outputs": [],
      "source": [
        "'''#!/usr/bin/env python\n",
        "  \n",
        "# import sys because we need to read and write data to STDIN and STDOUT\n",
        "import sys\n",
        "import os\n",
        "\n",
        "with open('/content/hadoop_data/badwords.txt', 'r') as f:\n",
        "  bad_words = {word.strip() for word in f}\n",
        "\n",
        "# reading entire line from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # to remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "      \n",
        "    # we are looping over the words array and printing the word\n",
        "    # with the count of 1 to the STDOUT\n",
        "    for badword in bad_words:\n",
        "      for i in range(0,len(words)):\n",
        "        try:\n",
        "          if(badword == words[i] + ' ' + words[i+1] + ' ' + words[i+2]):\n",
        "            print('%s %s %s\\t%s' % (words[i], words[i+1], words[i+2], 1))\n",
        "        except:\n",
        "          pass\n",
        "        try:\n",
        "          if(badword == words[i] + ' ' + words[i+1]):\n",
        "            print('%s %s\\t%s' % (words[i], words[i+1], 1))\n",
        "        except:\n",
        "          pass\n",
        "          # write the results to STDOUT (standard output);\n",
        "          # what we output here will be the input for the\n",
        "          # Reduce step, i.e. the input for reducer.py\n",
        "        if badword == words[i]:\n",
        "          print('%s\\t%s' % (words[i], 1))'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducer"
      ],
      "metadata": {
        "id": "h6BwkO53wnZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "######################## REDUCER ############################!\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    word, count = line.split('\\t', 1)\n",
        "\n",
        "    # convert count (currently a string) to int\n",
        "    try:\n",
        "        count = int(count)\n",
        "    except ValueError:\n",
        "        # count was not a number, so silently\n",
        "        # ignore/discard this line\n",
        "        continue\n",
        "\n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    # by key (here: word) before it is passed to the reducer\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            # write result to STDOUT\n",
        "            print('%s\\t%s' % (current_word, current_count))\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "    print('%s\\t%s' % (current_word, current_count))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "pf6R3kfk9jXA",
        "outputId": "478711e2-0c7d-485f-ff97-9f8ebaf1c5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n######################## REDUCER ############################\\nfrom operator import itemgetter\\nimport sys\\n\\ncurrent_word = None\\ncurrent_count = 0\\nword = None\\n\\n# input comes from STDIN\\nfor line in sys.stdin:\\n    # remove leading and trailing whitespace\\n    line = line.strip()\\n\\n    # parse the input we got from mapper.py\\n    word, count = line.split('\\t', 1)\\n\\n    # convert count (currently a string) to int\\n    try:\\n        count = int(count)\\n    except ValueError:\\n        # count was not a number, so silently\\n        # ignore/discard this line\\n        continue\\n\\n    # this IF-switch only works because Hadoop sorts map output\\n    # by key (here: word) before it is passed to the reducer\\n    if current_word == word:\\n        current_count += count\\n    else:\\n        if current_word:\\n            # write result to STDOUT\\n            print('%s\\t%s' % (current_word, current_count))\\n        current_count = count\\n        current_word = word\\n\\n# do not forget to output the last word if needed!\\nif current_word == word:\\n    print('%s\\t%s' % (current_word, current_count))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}